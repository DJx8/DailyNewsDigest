{"cells":[{"cell_type":"code","execution_count":1,"id":"da7d0351","metadata":{"vscode":{"languageId":"plaintext"},"colab":{"base_uri":"https://localhost:8080/"},"id":"da7d0351","executionInfo":{"status":"ok","timestamp":1751695858477,"user_tz":-330,"elapsed":47461,"user":{"displayName":"DhananJay Vidyasagar","userId":"15279736691954369572"}},"outputId":"20afb264-d84f-40db-a83d-bd7b472f2cfd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting newspaper3k\n","  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.2.1)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n","Collecting cssselect>=0.9.2 (from newspaper3k)\n","  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n","Collecting feedparser>=5.2.1 (from newspaper3k)\n","  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n","Collecting tldextract>=2.0.1 (from newspaper3k)\n","  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n","Collecting feedfinder2>=0.0.4 (from newspaper3k)\n","  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jieba3k>=0.35.1 (from newspaper3k)\n","  Downloading jieba3k-0.35.1.zip (7.4 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n","Collecting tinysegmenter==0.3 (from newspaper3k)\n","  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n","Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.6.15)\n","Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n","  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n","Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n","Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n","Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n","  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=a539463a88aae588278313fb2962b12740c4dd343753a621d6eb1f7f1ee59cfc\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n","  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=39c1cd3aadf1ee2bc9ef4a87c70f193034478aefed36439345a010af8ebf39f5\n","  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n","  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=b4238171e86ad2a4212f2f7d2952906ba8d12001d2e650c92be2d95da5d168e9\n","  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=29f403129a6a94decfed2fa2174c538f45a138c4dea2bcea1f2164d9ad5c2d7e\n","  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n","Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n","Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n","Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n","Collecting lxml_html_clean\n","  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.4.0)\n","Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n","Installing collected packages: lxml_html_clean\n","Successfully installed lxml_html_clean-0.4.2\n","Collecting newspaper\n","  Downloading newspaper-0.1.0.7.tar.gz (176 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n","\u001b[31mâ•°â”€>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install newspaper3k scikit-learn nltk networkx joblib\n","!pip install lxml_html_clean\n","!pip install newspaper"]},{"cell_type":"code","execution_count":2,"id":"464186e8","metadata":{"vscode":{"languageId":"plaintext"},"colab":{"base_uri":"https://localhost:8080/"},"id":"464186e8","executionInfo":{"status":"ok","timestamp":1751695964988,"user_tz":-330,"elapsed":106507,"user":{"displayName":"DhananJay Vidyasagar","userId":"15279736691954369572"}},"outputId":"7a6c8f6e-5da5-4643-f83d-0a03722fe4cc"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["   category                                               text\n","1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n","2  Business  Oil and Economy Cloud Stocks' Outlook (Reuters...\n","3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n","4  Business  Oil prices soar to all-time record, posing new...\n","5  Business  Stocks End Up, But Near Year Lows (Reuters). R...\n","              precision    recall  f1-score   support\n","\n","    Business       0.84      0.85      0.84      5872\n","    Sci/Tech       0.85      0.85      0.85      5807\n","      Sports       0.94      0.97      0.95      5836\n","       World       0.91      0.88      0.89      6045\n","\n","    accuracy                           0.88     23560\n","   macro avg       0.88      0.88      0.88     23560\n","weighted avg       0.88      0.88      0.88     23560\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['news_classifier.joblib']"]},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import PorterStemmer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import classification_report\n","from joblib import dump, load\n","\n","from newspaper import Article\n","from sklearn.metrics.pairwise import cosine_similarity\n","import networkx as nx\n","\n","# Download NLTK resources if not already done\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab') # Download the missing resource\n","\n","# Initialize stopwords and stemmer\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","# Clean text function: keeps only sentences with at least 8 cleaned words\n","def clean_text(text):\n","    text = str(text).lower()\n","    sentences = sent_tokenize(text)\n","    cleaned_sentences = []\n","\n","    for sentence in sentences:\n","        tokens = word_tokenize(sentence)\n","        tokens = [t for t in tokens if t.isalpha()]\n","        tokens = [t for t in tokens if t not in stop_words]\n","        tokens = [stemmer.stem(t) for t in tokens]\n","\n","        if len(tokens) >= 8:\n","            cleaned_sentences.append(' '.join(tokens))\n","\n","    return ' '.join(cleaned_sentences)\n","\n","# Load dataset\n","df = pd.read_csv(\"/content/drive/MyDrive/NLP/Project/Dataset/train.csv\")\n","df.columns = ['class_id', 'title', 'description']\n","\n","# Map class IDs to category labels\n","category_map = {\n","    1: 'World',\n","    2: 'Sports',\n","    3: 'Business',\n","    4: 'Sci/Tech'\n","}\n","df['category'] = df['class_id'].map(category_map)\n","\n","# Combine title and description\n","df['text'] = df['title'].fillna('') + \". \" + df['description'].fillna('')\n","\n","# Apply cleaning\n","df['cleaned'] = df['text'].apply(clean_text)\n","\n","# Remove rows with empty cleaned text\n","df = df[df['cleaned'].str.strip() != '']\n","\n","# Show a sample\n","print(df[['category', 'text']].head())\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(df['cleaned'], df['category'], test_size=0.2, random_state=42)\n","\n","# Pipeline with TF-IDF and Naive Bayes\n","pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer(max_features=5000)),\n","    ('nb', MultinomialNB())\n","])\n","\n","# Train model\n","pipeline.fit(X_train, y_train)\n","\n","# Evaluate model\n","y_pred = pipeline.predict(X_test)\n","print(classification_report(y_test, y_pred))\n","\n","# Save model\n","dump(pipeline, \"news_classifier.joblib\")"]},{"cell_type":"code","execution_count":5,"id":"a5a2b34d","metadata":{"vscode":{"languageId":"plaintext"},"colab":{"base_uri":"https://localhost:8080/"},"id":"a5a2b34d","executionInfo":{"status":"ok","timestamp":1751696118631,"user_tz":-330,"elapsed":16290,"user":{"displayName":"DhananJay Vidyasagar","userId":"15279736691954369572"}},"outputId":"137ad840-9b04-4063-eb2e-a33c733ef500"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“… Enter date (YYYY-MM-DD): 2025-07-05\n","\n","ğŸ” Fetching top 5 articles for **World** on 2025-07-05...\n","\n","1. ğŸ“° Title: Pakistan, U.S. conclude critical round of trade negotiations\n","   ğŸ”— URL: https://www.thehindu.com/news/international/pakistan-us-conclude-critical-round-of-trade-negotiations/article69775625.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Pakistan and the U.S. have concluded a critical round of trade negotiations, reaching an understanding on a deal that could shape the future of the countryâ€™s key export sectors, a media report said on Saturday.\n","\n","2. ğŸ“° Title: Azerbaijan signs $2 billion investment agreement with Pakistan\n","   ğŸ”— URL: https://www.thehindu.com/news/international/azerbaijan-signs-2-billion-investment-agreement-with-pakistan/article69775678.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Pakistan and Azerbaijan have signed an agreement for investment worth $2 billion in diverse sectors of Pakistanâ€™s economy. The document was signed by Deputy Prime Minister/Foreign Minister Ishaq Dar and Azerbaijanâ€™s Minister for Economy Mikayil Jabbarov in Khankendi, Azerbaijan.\n","\n","3. ğŸ“° Title: Trump says Iran has not agreed to inspections, give up enrichment\n","   ğŸ”— URL: https://www.thehindu.com/news/international/trump-says-iran-has-not-agreed-to-inspections-give-up-enrichment/article69775640.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","U.S. President Donald Trump said on Friday (July 4, 2025) that Iran had not agreed to inspections of its nuclear programme or to give up enriching uranium. \"I would think they'd have to start at a different location.\n","\n","ğŸ” Fetching top 5 articles for **Business** on 2025-07-05...\n","\n","1. ğŸ“° Title: What is U.S. President Trumpâ€™s One Big Beautiful Bill all about? | Explained\n","   ğŸ”— URL: https://www.thehindu.com/business/Economy/what-is-president-trumps-one-big-beautiful-bill-all-about-explained/article69773008.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","The story so far: U.S. President Donald Trump signed into law the â€˜One Big Beautiful Billâ€™ on U.S.â€™ Independence Day on July 4, 2025. This is provided the individual income does not exceed $150,000.\n","\n","2. ğŸ“° Title: Microsoft closes its Pakistan office after 25 years\n","   ğŸ”— URL: https://www.thehindu.com/sci-tech/technology/microsoft-closes-its-pakistan-office-after-25-years/article69775424.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Tech giant Microsoft has announced it will shut down its limited operations in Pakistan as part of its global strategy to reduce its workforce, which various stakeholders termed on Friday as a â€œtroubling signâ€ for the country's economy.\n","\n","3. ğŸ“° Title: Google's AI Overviews hit by EU antitrust complaint from independent publishers\n","   ğŸ”— URL: https://www.thehindu.com/sci-tech/technology/googles-ai-overviews-hit-by-eu-antitrust-complaint-from-independent-publishers/article69775412.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Google's AI Overviews are AI-generated summaries that appear above traditional hyperlinks to relevant webpages and are shown to users in more than 100 countries. It began adding advertisements to AI Overviews last May.\n","\n","ğŸ” Fetching top 5 articles for **Sports** on 2025-07-05...\n","\n","1. ğŸ“° Title: Club World Cup | Chelsea advances to semifinals with 2-1 win over Palmeiras\n","   ğŸ”— URL: https://www.thehindu.com/sport/football/club-world-cup-chelsea-advances-to-semifinals-with-2-1-win-over-palmeiras/article69775566.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Chelsea scored the go-ahead goal on Malo Gusto's 83rd-minute shot that went in after a pair of deflections, beating Palmeiras 2-1 on Friday night for a spot in the Club World Cup semifinals. And it's beautiful from ALL ANGLES!\n","\n","2. ğŸ“° Title: Wimbledon 2025: World No. 1 Aryna Sabalenka holds off home favourite Emma Raducanu at Centre Court\n","   ğŸ”— URL: https://www.thehindu.com/sport/tennis/wimbledon-2025-world-no-1-aryna-sabalenka-holds-off-home-favourite-emma-raducanu-at-centre-court/article69775548.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Top-ranked Aryna Sabalenka staved off an upset bid by resurgent Emma Raducanu at Wimbledon by beating the home favourite 7-6(6) 6-4 in the third-round at a raucous Centre Court. when she missed.\n","\n","3. ğŸ“° Title: IND Women vs ENG Women third T20I: England edges India by five runs in thriller to keep series alive\n","   ğŸ”— URL: https://www.thehindu.com/sport/cricket/ind-women-vs-eng-women-third-t20i-england-edges-india-by-five-runs-in-thriller-to-keep-series-alive/article69775492.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","India's batting faltered at the death as England held their nerves to pull off a narrow five-run win in a thrilling third women's T20 International in London, keeping the series alive. The dew factor also came in.\n","\n","ğŸ” Fetching top 5 articles for **Sci/Tech** on 2025-07-05...\n","\n","1. ğŸ“° Title: Italian lawmakers seek answers from government on spyware scandal\n","   ğŸ”— URL: https://www.thehindu.com/sci-tech/technology/italian-lawmakers-seek-answers-from-government-on-spyware-scandal/article69775427.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Italian lawmakers have formally asked the government whether it spied on journalists, possibly using technology supplied by U.S.-based spyware maker Paragon, a source familiar with the matter said on Friday. Mantovano did not immediately respond to a request for comment.\n","\n","2. ğŸ“° Title: EU sticks with timeline for AI rules\n","   ğŸ”— URL: https://www.thehindu.com/sci-tech/technology/eu-sticks-with-timeline-for-ai-rules/article69775426.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","The European Union's landmark rules on artificial intelligence will be rolled out according to the legal timeline in the legislation, the European Commission said on Friday, dismissing calls from some companies and countries for a pause.\n","\n","3. ğŸ“° Title: Microsoft closes its Pakistan office after 25 years\n","   ğŸ”— URL: https://www.thehindu.com/sci-tech/technology/microsoft-closes-its-pakistan-office-after-25-years/article69775424.ece\n","   ğŸ“‚ Predicted Category: World\n","   ğŸ“ Summary:\n","Tech giant Microsoft has announced it will shut down its limited operations in Pakistan as part of its global strategy to reduce its workforce, which various stakeholders termed on Friday as a â€œtroubling signâ€ for the country's economy.\n"]}],"source":["import feedparser\n","from datetime import datetime\n","from newspaper import Article\n","from joblib import load\n","import nltk\n","import networkx as nx\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Download NLTK resources if not present\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Load classifier from earlier model\n","clf = load(\"news_classifier.joblib\")\n","\n","# Setup NLP tools\n","stop_words = set(stopwords.words('english'))\n","stemmer = PorterStemmer()\n","\n","def clean_text(text):\n","    text = str(text).lower()\n","    sentences = sent_tokenize(text)\n","    cleaned_sentences = []\n","    for sentence in sentences:\n","        tokens = word_tokenize(sentence)\n","        tokens = [t for t in tokens if t.isalpha()]\n","        tokens = [t for t in tokens if t not in stop_words]\n","        tokens = [stemmer.stem(t) for t in tokens]\n","    return ' '.join(cleaned_sentences)\n","\n","def summarize(text, top_n=2, max_words=40):\n","    sentences = sent_tokenize(text)\n","    if len(sentences) <= top_n:\n","        return ' '.join(sentences)\n","\n","    tfidf = TfidfVectorizer().fit_transform(sentences)\n","    sim_matrix = cosine_similarity(tfidf)\n","    nx_graph = nx.from_numpy_array(sim_matrix)\n","    scores = nx.pagerank(nx_graph)\n","\n","    ranked = sorted(((scores[i], s, i) for i, s in enumerate(sentences)), reverse=True)\n","\n","    summary = []\n","    total_words = 0\n","    for _, sentence, idx in sorted(ranked, key=lambda x: x[2]):\n","        word_count = len(sentence.split())\n","        if total_words + word_count <= max_words:\n","            summary.append(sentence)\n","            total_words += word_count\n","        if len(summary) >= top_n or total_words >= max_words:\n","            break\n","    return ' '.join(summary)\n","\n","def fetch_article_details(url):\n","    try:\n","        article = Article(url)\n","        article.download()\n","        article.parse()\n","        title, text = article.title, article.text\n","        cleaned = clean_text(title + \". \" + text)\n","        predicted_category = clf.predict([cleaned])[0]\n","        summary = summarize(text)\n","        return {\n","            \"title\": title,\n","            \"url\": url,\n","            \"predicted_category\": predicted_category,\n","            \"summary\": summary\n","        }\n","    except Exception as e:\n","        return None\n","\n","def fetch_top_articles_from_rss(date_str, category_rss_path):\n","    url = f\"https://www.thehindu.com/{category_rss_path}/?service=rss\"\n","    feed = feedparser.parse(url)\n","    target_date = datetime.strptime(date_str, \"%Y-%m-%d\").date()\n","\n","    articles = []\n","    for entry in feed.entries:\n","        if hasattr(entry, 'published_parsed'):\n","            pub_date = datetime(*entry.published_parsed[:6]).date()\n","            if pub_date == target_date:\n","                details = fetch_article_details(entry.link)\n","                if details:\n","                    articles.append(details)\n","        if len(articles) >= 5:\n","            break\n","    return articles\n","\n","def main():\n","    date_input = input(\"ğŸ“… Enter date (YYYY-MM-DD): \").strip()\n","\n","    category_mapping = {\n","        \"World\": \"news/international\",\n","        \"Business\": \"business\",\n","        \"Sports\": \"sport\",\n","        \"Sci/Tech\": \"sci-tech\"\n","    }\n","\n","    final_results = {}\n","\n","    for category_name, rss_path in category_mapping.items():\n","        print(f\"\\nğŸ” Fetching top 5 articles for **{category_name}** on {date_input}...\")\n","        articles = fetch_top_articles_from_rss(date_input, rss_path)\n","        final_results[category_name] = articles\n","\n","        for idx, article in enumerate(articles, 1):\n","            print(f\"\\n{idx}. ğŸ“° Title: {article['title']}\")\n","            print(f\"   ğŸ”— URL: {article['url']}\")\n","            print(f\"   ğŸ“‚ Predicted Category: {article['predicted_category']}\")\n","            print(f\"   ğŸ“ Summary:\\n{article['summary'][:300]}{'...' if len(article['summary']) > 300 else ''}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","source":["### Getting the model"],"metadata":{"id":"ykDEkE0x-F0A"},"id":"ykDEkE0x-F0A"},{"cell_type":"code","source":["# from joblib import dump\n","\n","# # Assuming 'pipeline' is your trained model\n","# dump(pipeline, \"news_classifier.joblib\")\n","# from google.colab import files\n","# files.download(\"news_classifier.joblib\")\n"],"metadata":{"id":"on1F8GUe-J2q","executionInfo":{"status":"ok","timestamp":1751695998469,"user_tz":-330,"elapsed":42,"user":{"displayName":"DhananJay Vidyasagar","userId":"15279736691954369572"}}},"id":"on1F8GUe-J2q","execution_count":4,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}